{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-salmon",
   "metadata": {},
   "source": [
    "# Lesson 7 & 8 - Training & Inference\n",
    "- 7강과 8강에서는 모델을 학습하고 추론하는 방법에 대해 알아보았습니다.\n",
    "- 이번 실습 자료에서는 다양한 Loss, Optimizer, Scheduler를 활용하는 방법을 알아봅니다.\n",
    "- 또한, Checkpoint, Early Stopping과 같은 학습을 도와주는 Callback 방법을 알아봅니다.\n",
    "- 그리고 Graident Accumulation 방법을 활용하여 학습을 진행해봅니다.\n",
    "## 0. Libraries & Configurations\n",
    "- 시각화에 필요한 라이브러리와 학습에 필요한 설정을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "changing-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "drawn-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dataset import MaskBaseDataset\n",
    "from model import *\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "img_root = '/opt/ml/input/data/train/images'  # 학습 이미지 폴더의 경로\n",
    "label_path = '/opt/ml/input/data/train/train.csv'  # 학습 메타파일의 경로\n",
    "\n",
    "# model_name = \"BaseModel\"  # 모델 이름\n",
    "model_name = \"MyModel\"  # 모델 이름\n",
    "use_pretrained = True  # pretrained-model의 사용 여부\n",
    "freeze_backbone = False  # classifier head 이 외 부분을 업데이트되지 않게 할 것인지 여부\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 18\n",
    "\n",
    "num_epochs = 5  # 학습할 epoch의 수\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_vgg\"  # 결과를 저장하는 폴더의 이름\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-earth",
   "metadata": {},
   "source": [
    "## 1. Loss\n",
    "- Image Classification에 사용되는 다양한 loss 함수들이 존재합니다. 각 loss 함수는 목적이 있고 풀고자 하는 문제에 맞게 적용을 해야합니다.\n",
    "- Cross Entropy Loss는 두 분포간의 불확실성을 최소화 하는 목적을 가진 분류에 사용되는 일반적인 손실함수입니다.\n",
    "- Focal Loss는 Imbalanced Data 문제를 해결하기 위한 손실함수입니다. [참고](https://arxiv.org/pdf/1708.02002.pdf)\n",
    "- Label Smoothing은 학습 데이터의 representation을 더 잘나타내는데 도움을 줍니다. [참고](https://arxiv.org/pdf/1906.02629.pdf)\n",
    "- F1 Loss는 F1 score 향상을 목적으로 하는 손실함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changing-december",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Cross Entropy Loss\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "revised-escape",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Focal Loss\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tired-structure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Label Smoothing Loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "raised-return",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- F1 Loss\n",
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ideal-worst",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-rendering",
   "metadata": {},
   "source": [
    "## 2. Optimizer\n",
    "- 파이토치는 코드를 간단히 수정하여 다양한 optimizer를 사용할 수 있습니다.\n",
    "- 또한 Model의 레이어마다 다른 learning rate를 적용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesser-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- model\n",
    "model_module = getattr(import_module(\"model\"), model_name)  # default: BaseModel\n",
    "model = model_module(\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "model = torch.nn.DataParallel(model) # 데이터 병렬처리. 다수의 GPU 사용가능하게. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neither-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SGD optimizer\n",
    "optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eastern-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-motivation",
   "metadata": {},
   "source": [
    "- 한 모델에 다른 learning rate를 적용시키기 위해 모델의 구조를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "checked-badge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda kv: 'module.fc' in kv[0], model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuing-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- optimizer: Different Learning Rates on different layers\n",
    "\n",
    "# features 레이어와 classifier 레이어에서 서로 다른 learning rate를 적용하여 optimizer를 정의할 수 있습니다.\n",
    "params = [v for k, v in list(filter(lambda kv: 'module.fc' not in kv[0], model.named_parameters()))]\n",
    "fc_params = [v for k, v in list(filter(lambda kv: 'module.fc' in kv[0], model.named_parameters()))]\n",
    "train_params = [{'params': params, 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                {'params': fc_params, 'lr': lr, 'weight_decay':5e-4}]\n",
    "optimizer = Adam(train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-commodity",
   "metadata": {},
   "source": [
    "## 3. Scheduler\n",
    "- Scheduler은 optimizer의 learning rate를 동적으로 변경시키는 기능을 합니다.\n",
    "- Optimizer과 Scheduler를 적절히 활용하면 모델이 좋은 성능으로 Fitting하는데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "satisfactory-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: StepLR\n",
    "# 지정된 step마다 learning rate를 감소시킵니다.\n",
    "scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contemporary-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: ReduceLROnPlateau\n",
    "# 성능이 향상되지 않을 때 learning rate를 줄입니다. patience=10은 10회 동안 성능 향상이 없을 경우입니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "environmental-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: CosineAnnealingLR\n",
    "# CosineAnnealing은 learning rate를 cosine 그래프처럼 변화시킵니다.\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-correspondence",
   "metadata": {},
   "source": [
    "## 4. Metric\n",
    "- Classification 성능을 표현할 때 다양한 평가지표가 있습니다.\n",
    "- Accuracy: 모델이 정확하게 예측한 객체의 비율\n",
    "- True Positive(TP): 실제 True인 정답을 True라고 예측 (정답)\n",
    "- False Positive(FP): 실제 False인 정답을 True라고 예측 (오답)\n",
    "- False Negative(FN): 실제 True인 정답을 False라고 예측 (오답)\n",
    "- True Negative(TN): 실제 False인 정답을 False라고 예측 (정답)\n",
    "- Precision(정밀도): TP / (TP + FP)\n",
    "- Recall(재현율): TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alleged-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aware-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Accuracy\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "focal-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Accuracy\n",
    "# Normalize를 안하면 맞춘 개수가 표시된다\n",
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "northern-latino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "following-count",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "legitimate-corpus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- f1 score\n",
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "criminal-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- f1 score (sklearn)\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-marker",
   "metadata": {},
   "source": [
    "## 5. Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vulnerable-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- dataset\n",
    "dataset_module = getattr(import_module(\"dataset\"), 'MaskBaseDataset')\n",
    "dataset = dataset_module(\n",
    "    data_dir=img_root,\n",
    ")\n",
    "num_classes = dataset.num_classes  # 18\n",
    "\n",
    "# -- augmentation\n",
    "transform_module = getattr(import_module(\"dataset\"), 'BaseAugmentation')\n",
    "transform = transform_module(\n",
    "    resize=[128, 96],\n",
    "    mean=dataset.mean,\n",
    "    std=dataset.std,\n",
    ")\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "\n",
    "# -- data_loader\n",
    "train_set, val_set = dataset.split_dataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "average-warren",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/ml')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path().resolve().parent.parent \n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "intimate-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/ml/output/all_df.pkl')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_path = BASE_DIR / 'output' / 'all_df.pkl'\n",
    "pkl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "comic-insertion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>filepath</th>\n",
       "      <th>filename</th>\n",
       "      <th>mask_type</th>\n",
       "      <th>age_category</th>\n",
       "      <th>category</th>\n",
       "      <th>cat_code</th>\n",
       "      <th>gender_code</th>\n",
       "      <th>mask_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>incorrect_mask.jpg</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>middle</td>\n",
       "      <td>(female, middle, incorrect)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>mask1.jpg</td>\n",
       "      <td>mask</td>\n",
       "      <td>middle</td>\n",
       "      <td>(female, middle, mask)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>mask2.jpg</td>\n",
       "      <td>mask</td>\n",
       "      <td>middle</td>\n",
       "      <td>(female, middle, mask)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>mask3.jpg</td>\n",
       "      <td>mask</td>\n",
       "      <td>middle</td>\n",
       "      <td>(female, middle, mask)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>mask4.jpg</td>\n",
       "      <td>mask</td>\n",
       "      <td>middle</td>\n",
       "      <td>(female, middle, mask)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path  \\\n",
       "4  000001  female  Asian   45  000001_female_Asian_45   \n",
       "3  000001  female  Asian   45  000001_female_Asian_45   \n",
       "6  000001  female  Asian   45  000001_female_Asian_45   \n",
       "2  000001  female  Asian   45  000001_female_Asian_45   \n",
       "0  000001  female  Asian   45  000001_female_Asian_45   \n",
       "\n",
       "                                            filepath            filename  \\\n",
       "4  /opt/ml/input/data/train/images/000001_female_...  incorrect_mask.jpg   \n",
       "3  /opt/ml/input/data/train/images/000001_female_...           mask1.jpg   \n",
       "6  /opt/ml/input/data/train/images/000001_female_...           mask2.jpg   \n",
       "2  /opt/ml/input/data/train/images/000001_female_...           mask3.jpg   \n",
       "0  /opt/ml/input/data/train/images/000001_female_...           mask4.jpg   \n",
       "\n",
       "   mask_type age_category                     category  cat_code  gender_code  \\\n",
       "4  incorrect       middle  (female, middle, incorrect)         0            0   \n",
       "3       mask       middle       (female, middle, mask)         1            0   \n",
       "6       mask       middle       (female, middle, mask)         1            0   \n",
       "2       mask       middle       (female, middle, mask)         1            0   \n",
       "0       mask       middle       (female, middle, mask)         1            0   \n",
       "\n",
       "   mask_code  \n",
       "4          0  \n",
       "3          1  \n",
       "6          1  \n",
       "2          1  \n",
       "0          1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(pkl_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "yellow-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- dataset\n",
    "dataset_module = getattr(import_module(\"dataset\"), 'MaskBaseDataset')\n",
    "dataset = dataset_module(\n",
    "    data_dir=img_root,\n",
    ")\n",
    "num_classes = dataset.num_classes  # 18\n",
    "\n",
    "# -- augmentation\n",
    "transform_module = getattr(import_module(\"dataset\"), 'BaseAugmentation')\n",
    "transform = transform_module(\n",
    "    resize=[128, 96],\n",
    "    mean=dataset.mean,\n",
    "    std=dataset.std,\n",
    ")\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "\n",
    "# -- data_loader\n",
    "train_set, val_set = dataset.split_dataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-healthcare",
   "metadata": {},
   "source": [
    "### 5.1 Callback - Checkpoint, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "painted-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n",
    "# 학습 코드에서 이어집니다.\n",
    "\n",
    "# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n",
    "patience = 10\n",
    "counter = 0\n",
    "# 학습 코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-guest",
   "metadata": {},
   "source": [
    "### 5.2 Training Method - Gradient Accumulation\n",
    "- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "central-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Gradient Accumulation\n",
    "accumulation_steps = 2\n",
    "# 학습코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-cuisine",
   "metadata": {},
   "source": [
    "### 5.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "royal-grass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](20/237) || training loss 2.639 || training accuracy 25.16% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](40/237) || training loss 2.14 || training accuracy 34.38% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](60/237) || training loss 1.858 || training accuracy 46.33% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](80/237) || training loss 1.683 || training accuracy 48.20% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](100/237) || training loss 1.527 || training accuracy 51.33% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](120/237) || training loss 1.361 || training accuracy 59.45% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](140/237) || training loss 1.277 || training accuracy 64.53% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](160/237) || training loss 1.093 || training accuracy 70.31% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](180/237) || training loss 0.9215 || training accuracy 74.30% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](200/237) || training loss 0.8257 || training accuracy 74.45% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](220/237) || training loss 0.7829 || training accuracy 76.33% || lr [1e-05, 0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 82.49%, loss: 0.61 || best acc : 82.49%, best loss: 0.61\n",
      "Epoch[1/5](20/237) || training loss 0.5917 || training accuracy 82.19% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](40/237) || training loss 0.557 || training accuracy 82.97% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](60/237) || training loss 0.5393 || training accuracy 83.44% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](80/237) || training loss 0.558 || training accuracy 83.44% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](100/237) || training loss 0.4914 || training accuracy 84.77% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](120/237) || training loss 0.5006 || training accuracy 85.39% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](140/237) || training loss 0.4994 || training accuracy 85.86% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](160/237) || training loss 0.4782 || training accuracy 85.31% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](180/237) || training loss 0.4869 || training accuracy 85.00% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](200/237) || training loss 0.4388 || training accuracy 85.47% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](220/237) || training loss 0.4385 || training accuracy 85.86% || lr [5e-06, 5e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 86.98%, loss: 0.41 || best acc : 86.98%, best loss: 0.41\n",
      "Epoch[2/5](20/237) || training loss 0.3538 || training accuracy 88.59% || lr [0.0, 0.0]\n",
      "Epoch[2/5](40/237) || training loss 0.4081 || training accuracy 86.41% || lr [0.0, 0.0]\n",
      "Epoch[2/5](60/237) || training loss 0.3918 || training accuracy 87.34% || lr [0.0, 0.0]\n",
      "Epoch[2/5](80/237) || training loss 0.3743 || training accuracy 87.73% || lr [0.0, 0.0]\n",
      "Epoch[2/5](100/237) || training loss 0.4039 || training accuracy 87.81% || lr [0.0, 0.0]\n",
      "Epoch[2/5](120/237) || training loss 0.3731 || training accuracy 87.58% || lr [0.0, 0.0]\n",
      "Epoch[2/5](140/237) || training loss 0.3752 || training accuracy 88.05% || lr [0.0, 0.0]\n",
      "Epoch[2/5](160/237) || training loss 0.4017 || training accuracy 87.11% || lr [0.0, 0.0]\n",
      "Epoch[2/5](180/237) || training loss 0.3619 || training accuracy 88.36% || lr [0.0, 0.0]\n",
      "Epoch[2/5](200/237) || training loss 0.4017 || training accuracy 86.33% || lr [0.0, 0.0]\n",
      "Epoch[2/5](220/237) || training loss 0.3754 || training accuracy 87.81% || lr [0.0, 0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 86.90%, loss: 0.41 || best acc : 86.98%, best loss: 0.41\n",
      "Epoch[3/5](20/237) || training loss 0.364 || training accuracy 87.81% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](40/237) || training loss 0.3624 || training accuracy 90.39% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](60/237) || training loss 0.3516 || training accuracy 88.75% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](80/237) || training loss 0.3304 || training accuracy 89.69% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](100/237) || training loss 0.3568 || training accuracy 87.89% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](120/237) || training loss 0.3141 || training accuracy 89.61% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](140/237) || training loss 0.3367 || training accuracy 87.97% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](160/237) || training loss 0.3561 || training accuracy 88.59% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](180/237) || training loss 0.326 || training accuracy 89.69% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](200/237) || training loss 0.3434 || training accuracy 89.53% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Epoch[3/5](220/237) || training loss 0.3196 || training accuracy 90.47% || lr [4.9999999999999996e-06, 4.999999999999999e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 89.97%, loss: 0.31 || best acc : 89.97%, best loss: 0.31\n",
      "Epoch[4/5](20/237) || training loss 0.2788 || training accuracy 90.23% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](40/237) || training loss 0.2648 || training accuracy 91.64% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](60/237) || training loss 0.2723 || training accuracy 90.70% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](80/237) || training loss 0.2298 || training accuracy 92.97% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](100/237) || training loss 0.2516 || training accuracy 91.72% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](120/237) || training loss 0.2214 || training accuracy 92.34% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](140/237) || training loss 0.211 || training accuracy 92.66% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](160/237) || training loss 0.205 || training accuracy 93.28% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](180/237) || training loss 0.2084 || training accuracy 93.05% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](200/237) || training loss 0.1671 || training accuracy 94.77% || lr [1e-05, 0.0001]\n",
      "Epoch[4/5](220/237) || training loss 0.2166 || training accuracy 92.66% || lr [1e-05, 0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 92.75%, loss: 0.21 || best acc : 92.75%, best loss: 0.21\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "#             current_lr = scheduler.get_last_lr()\n",
    "            current_lr = scheduler.get_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "        \n",
    "        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accredited-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 128, 96])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 18])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# MyModel\n",
    "\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(outs.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "intense-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 3, 128, 96])\n",
      "torch.Size([24])\n",
      "torch.Size([24, 18])\n",
      "torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "# BaseModel\n",
    "\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(outs.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-artwork",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-place",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opposed-booth",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "- [sumni blog post](https://sumniya.tistory.com/26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "above-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "neural-bathroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/ml')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/opt/ml/').resolve()\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "external-treat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/ml/input/data/eval')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "\n",
    "# test_dir = '/mnt/ssd/data/mask_final/public'\n",
    "test_dir = BASE_DIR / 'input' / 'data' / 'eval'\n",
    "test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "grand-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "victorian-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "naughty-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_mymodel.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-forest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
