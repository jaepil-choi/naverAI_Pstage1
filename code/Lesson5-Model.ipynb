{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-blackjack",
   "metadata": {},
   "source": [
    "## Lesson 5 - Model\n",
    " - 이번 실습 자료에서는 강의시간에 다루었던 파이토치 모델을 정의하는 방법에 대해 실습하겠습니다.\n",
    " - 파이토치 모델은 기본적으로 `nn.Module` 클래스를 상속하여 사용합니다.\n",
    "     - [공식문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)에 따르면 `nn.Module` 은 다음과 같은 기능을 합니다\n",
    "     ```\n",
    "     Base class for all neural network modules.\n",
    "     Your models should also subclass this class.\n",
    "     Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informal-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "official-leisure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-there",
   "metadata": {},
   "source": [
    "### 모델 디버깅\n",
    " - 파이토치 모델들은 다음과 같읕 방법들을 통해 파라미터를 눈으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-shooting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight         - size: torch.Size([3, 1, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0798,  0.0982, -0.0689],\n",
      "          [ 0.2807,  0.1054, -0.0321],\n",
      "          [ 0.2718, -0.3276, -0.1768]]],\n",
      "\n",
      "\n",
      "        [[[-0.3259,  0.1706,  0.2598],\n",
      "          [-0.0917,  0.1226, -0.2796],\n",
      "          [ 0.1066, -0.1066,  0.1998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1872,  0.3121,  0.2176],\n",
      "          [-0.2487,  0.3190, -0.2874],\n",
      "          [-0.3131, -0.3031, -0.1055]]]], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "conv1.bias           - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([ 0.3161,  0.2642, -0.1567], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "bn1.weight           - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "bn1.bias             - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "conv2.weight         - size: torch.Size([5, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0456,  0.1448,  0.1720],\n",
      "          [-0.0627,  0.0675, -0.1372],\n",
      "          [-0.1072, -0.0426,  0.0363]],\n",
      "\n",
      "         [[-0.0362,  0.0319,  0.0322],\n",
      "          [-0.1627,  0.0131, -0.0055],\n",
      "          [-0.1228, -0.0489,  0.0771]],\n",
      "\n",
      "         [[ 0.1694,  0.0212, -0.1248],\n",
      "          [ 0.0046,  0.0103,  0.1032],\n",
      "          [ 0.0688, -0.1434,  0.0993]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1828, -0.1506, -0.1583],\n",
      "          [-0.0923,  0.0032,  0.1263],\n",
      "          [-0.1156, -0.1579,  0.0054]],\n",
      "\n",
      "         [[ 0.0294, -0.0076,  0.1103],\n",
      "          [ 0.1346, -0.1437, -0.0381],\n",
      "          [-0.0449,  0.1780, -0.0115]],\n",
      "\n",
      "         [[-0.0165,  0.0484, -0.0069],\n",
      "          [ 0.0047, -0.1178,  0.1819],\n",
      "          [ 0.0920, -0.0498,  0.1374]]],\n",
      "\n",
      "\n",
      "        [[[-0.0582, -0.1374,  0.1570],\n",
      "          [-0.0325,  0.1055, -0.1754],\n",
      "          [-0.1515,  0.1041, -0.0061]],\n",
      "\n",
      "         [[ 0.0625,  0.0471,  0.1248],\n",
      "          [ 0.0901, -0.0894,  0.1716],\n",
      "          [-0.1353, -0.0003, -0.1382]],\n",
      "\n",
      "         [[-0.0720,  0.1912, -0.1019],\n",
      "          [-0.0254, -0.0967, -0.0996],\n",
      "          [ 0.1455,  0.0228,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0600, -0.1852,  0.0199],\n",
      "          [ 0.0378,  0.1790, -0.1183],\n",
      "          [-0.0397, -0.1638,  0.1742]],\n",
      "\n",
      "         [[ 0.0322,  0.0428, -0.1033],\n",
      "          [ 0.1313, -0.1039,  0.1383],\n",
      "          [-0.0491, -0.0908,  0.1613]],\n",
      "\n",
      "         [[ 0.0686,  0.0127,  0.0056],\n",
      "          [ 0.0385,  0.0936, -0.0361],\n",
      "          [ 0.0330,  0.0966,  0.1037]]],\n",
      "\n",
      "\n",
      "        [[[-0.0706, -0.1760, -0.0045],\n",
      "          [-0.1717,  0.0988,  0.0046],\n",
      "          [ 0.1346,  0.1827,  0.1256]],\n",
      "\n",
      "         [[ 0.0074, -0.1760, -0.1271],\n",
      "          [ 0.0053,  0.0478, -0.0211],\n",
      "          [ 0.0400, -0.0152, -0.1185]],\n",
      "\n",
      "         [[ 0.1215, -0.0125, -0.1535],\n",
      "          [-0.1118, -0.1744,  0.0029],\n",
      "          [-0.1470,  0.0725, -0.0906]]]], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. using named_parameters()\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"{param:20} - size: {weight.size()}\")\n",
    "    print(weight)\n",
    "    print(\"-\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dangerous-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.0798,  0.0982, -0.0689],\n",
      "          [ 0.2807,  0.1054, -0.0321],\n",
      "          [ 0.2718, -0.3276, -0.1768]]],\n",
      "\n",
      "\n",
      "        [[[-0.3259,  0.1706,  0.2598],\n",
      "          [-0.0917,  0.1226, -0.2796],\n",
      "          [ 0.1066, -0.1066,  0.1998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1872,  0.3121,  0.2176],\n",
      "          [-0.2487,  0.3190, -0.2874],\n",
      "          [-0.3131, -0.3031, -0.1055]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3161,  0.2642, -0.1567], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. directly access with member variable\n",
    "print(model.conv1.weight)\n",
    "print(model.conv1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-century",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "numerous-spank",
   "metadata": {},
   "source": [
    "### 학습된 모델 저장하기\n",
    " - `torch.save(model.state_dict(), save_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "massive-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saving success at ./runs/best.pth\n",
      "Saved models : ['best.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_folder = \"./runs/\"\n",
    "save_path = os.path.join(save_folder, \"best.pth\")   # ./runs/best.pth\n",
    "os.makedirs(save_folder, exist_ok=True)  \n",
    "\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saving success at {save_path}\")\n",
    "print(f\"Saved models : {os.listdir(save_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-dynamics",
   "metadata": {},
   "source": [
    "### 저장된 모델 불러오기\n",
    " - model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electrical-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading success from ./runs/best.pth\n"
     ]
    }
   ],
   "source": [
    "new_model = Model()\n",
    "new_model.load_state_dict(torch.load(save_path))\n",
    "print(f\"Model loading success from {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-antibody",
   "metadata": {},
   "source": [
    "#### 저장된 모델이 잘 불러와졌는지 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedicated-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter conv1.weight    from trained model and loaded model is equal? -> True\n",
      "parameter conv1.bias      from trained model and loaded model is equal? -> True\n",
      "parameter bn1.weight      from trained model and loaded model is equal? -> True\n",
      "parameter bn1.bias        from trained model and loaded model is equal? -> True\n",
      "parameter conv2.weight    from trained model and loaded model is equal? -> True\n"
     ]
    }
   ],
   "source": [
    "for (name, trained_weight), (_, saved_weight) in zip(model.named_parameters(), new_model.named_parameters()):\n",
    "    is_equal = torch.equal(trained_weight, saved_weight)\n",
    "    print(f\"parameter {name:15} from trained model and loaded model is equal? -> {is_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-accuracy",
   "metadata": {},
   "source": [
    "#### state_dict() 이 무엇인가요?\n",
    " - 모델의 저장과 로딩에 `state_dict()` 을 사용하는데, 기능이 무엇인가요?\n",
    " - 기본적으로 위에서 살펴본 `.named_parameters()` 와 매우 유사합니다\n",
    " - model parameter 를 Key 로 가지고, model weights 를 Value 로 가지는 파이썬 딕셔너리일 뿐입니다. \n",
    "   (정확한 Type 은 파이썬 내장 라이브러리 collections.OrderDict 입니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dental-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight         - size: torch.Size([3, 1, 3, 3])\n",
      "tensor([[[[ 0.0798,  0.0982, -0.0689],\n",
      "          [ 0.2807,  0.1054, -0.0321],\n",
      "          [ 0.2718, -0.3276, -0.1768]]],\n",
      "\n",
      "\n",
      "        [[[-0.3259,  0.1706,  0.2598],\n",
      "          [-0.0917,  0.1226, -0.2796],\n",
      "          [ 0.1066, -0.1066,  0.1998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1872,  0.3121,  0.2176],\n",
      "          [-0.2487,  0.3190, -0.2874],\n",
      "          [-0.3131, -0.3031, -0.1055]]]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "conv1.bias           - size: torch.Size([3])\n",
      "tensor([ 0.3161,  0.2642, -0.1567])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.weight           - size: torch.Size([3])\n",
      "tensor([1., 1., 1.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.bias             - size: torch.Size([3])\n",
      "tensor([0., 0., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.running_mean     - size: torch.Size([3])\n",
      "tensor([0., 0., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.running_var      - size: torch.Size([3])\n",
      "tensor([1., 1., 1.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.num_batches_tracked - size: torch.Size([])\n",
      "tensor(0)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "conv2.weight         - size: torch.Size([5, 3, 3, 3])\n",
      "tensor([[[[ 0.0456,  0.1448,  0.1720],\n",
      "          [-0.0627,  0.0675, -0.1372],\n",
      "          [-0.1072, -0.0426,  0.0363]],\n",
      "\n",
      "         [[-0.0362,  0.0319,  0.0322],\n",
      "          [-0.1627,  0.0131, -0.0055],\n",
      "          [-0.1228, -0.0489,  0.0771]],\n",
      "\n",
      "         [[ 0.1694,  0.0212, -0.1248],\n",
      "          [ 0.0046,  0.0103,  0.1032],\n",
      "          [ 0.0688, -0.1434,  0.0993]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1828, -0.1506, -0.1583],\n",
      "          [-0.0923,  0.0032,  0.1263],\n",
      "          [-0.1156, -0.1579,  0.0054]],\n",
      "\n",
      "         [[ 0.0294, -0.0076,  0.1103],\n",
      "          [ 0.1346, -0.1437, -0.0381],\n",
      "          [-0.0449,  0.1780, -0.0115]],\n",
      "\n",
      "         [[-0.0165,  0.0484, -0.0069],\n",
      "          [ 0.0047, -0.1178,  0.1819],\n",
      "          [ 0.0920, -0.0498,  0.1374]]],\n",
      "\n",
      "\n",
      "        [[[-0.0582, -0.1374,  0.1570],\n",
      "          [-0.0325,  0.1055, -0.1754],\n",
      "          [-0.1515,  0.1041, -0.0061]],\n",
      "\n",
      "         [[ 0.0625,  0.0471,  0.1248],\n",
      "          [ 0.0901, -0.0894,  0.1716],\n",
      "          [-0.1353, -0.0003, -0.1382]],\n",
      "\n",
      "         [[-0.0720,  0.1912, -0.1019],\n",
      "          [-0.0254, -0.0967, -0.0996],\n",
      "          [ 0.1455,  0.0228,  0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0600, -0.1852,  0.0199],\n",
      "          [ 0.0378,  0.1790, -0.1183],\n",
      "          [-0.0397, -0.1638,  0.1742]],\n",
      "\n",
      "         [[ 0.0322,  0.0428, -0.1033],\n",
      "          [ 0.1313, -0.1039,  0.1383],\n",
      "          [-0.0491, -0.0908,  0.1613]],\n",
      "\n",
      "         [[ 0.0686,  0.0127,  0.0056],\n",
      "          [ 0.0385,  0.0936, -0.0361],\n",
      "          [ 0.0330,  0.0966,  0.1037]]],\n",
      "\n",
      "\n",
      "        [[[-0.0706, -0.1760, -0.0045],\n",
      "          [-0.1717,  0.0988,  0.0046],\n",
      "          [ 0.1346,  0.1827,  0.1256]],\n",
      "\n",
      "         [[ 0.0074, -0.1760, -0.1271],\n",
      "          [ 0.0053,  0.0478, -0.0211],\n",
      "          [ 0.0400, -0.0152, -0.1185]],\n",
      "\n",
      "         [[ 0.1215, -0.0125, -0.1535],\n",
      "          [-0.1118, -0.1744,  0.0029],\n",
      "          [-0.1470,  0.0725, -0.0906]]]])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param, weight in model.state_dict().items():\n",
    "    print(f\"{param:20} - size: {weight.size()}\")\n",
    "    print(weight)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changed-louisiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.state_dict() type is : <class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "print(f\"model.state_dict() type is : {type(model.state_dict())}\")\n",
    "type(model.state_dict()) == OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-variation",
   "metadata": {},
   "source": [
    "#### `named_parameters()` 을 안쓰고 `state_dict()` 을 사용하는 이유가 무언인가요? (둘이 뭐가 다른가요)\n",
    " - `named_parameters()` : returns only parameters\n",
    " - `state_dict()`: returns both parameters and buffers (e.g. BN runnin_mean, running_var)\n",
    " \n",
    " [Reference](https://stackoverflow.com/a/54747245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elegant-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1.weight', 'conv1.bias', 'bn1.weight', 'bn1.bias', 'conv2.weight']\n",
      "\n",
      "['conv1.weight',\n",
      " 'conv1.bias',\n",
      " 'bn1.weight',\n",
      " 'bn1.bias',\n",
      " 'bn1.running_mean',\n",
      " 'bn1.running_var',\n",
      " 'bn1.num_batches_tracked',\n",
      " 'conv2.weight']\n"
     ]
    }
   ],
   "source": [
    "pprint([name for (name, param) in model.named_parameters()])  # named_parameters() : returns only parameters\n",
    "print()\n",
    "pprint(list(model.state_dict().keys()))  # state_dict(): retuns both parameters and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-albania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "searching-surfing",
   "metadata": {},
   "source": [
    "### CPU vs GPU\n",
    " - DL 모델은 다양한 프로세서(CPU, GPU, TPU) 를 사용하여 학습을 할 수 있습니다.\n",
    " - 따라서, 특정 프로세서에서 학습을 진행하고 싶은 경우 명시적으로 지정해주어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-juice",
   "metadata": {},
   "source": [
    "#### cpu()\n",
    "Moves all model parameters and buffers to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "settled-intention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "for weight in model.parameters():\n",
    "    print(f\"model device: {weight.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-employer",
   "metadata": {},
   "source": [
    "#### cuda()\n",
    "Moves all model parameters and buffers to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "careful-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "for weight in model.parameters():\n",
    "    print(f\"model device: {weight.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-victor",
   "metadata": {},
   "source": [
    "#### to()\n",
    "Moves and/or casts the parameters and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "built-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set model device to cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "\n",
      "Set model device to cuda\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device_options = ['cpu', 'cuda']\n",
    "for device_option in device_options:\n",
    "    device = torch.device(device_option)\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Set model device to {device_option}\")\n",
    "    for weight in model.parameters():\n",
    "        print(f\"model device: {weight.device}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "solved-sterling",
   "metadata": {},
   "source": [
    "### forward\n",
    " - nn.Module 을 상속한 객체를 직접 호출할 때 수행하는 연산을 정의합니다.\n",
    " - `model(input)` 을 통해 모델의 예측값을 계산할 수 있습니다.\n",
    " - Defines the computation performed at every call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "productive-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: torch.Size([1, 5, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 2.2994e-01, 1.3038e-01, 2.1326e-01, 0.0000e+00,\n",
       "           8.5100e-02, 0.0000e+00, 1.1047e-01],\n",
       "          [1.1781e-03, 0.0000e+00, 0.0000e+00, 1.6044e-01, 0.0000e+00,\n",
       "           0.0000e+00, 3.4512e-02, 0.0000e+00],\n",
       "          [4.2940e-02, 0.0000e+00, 1.4471e-01, 0.0000e+00, 6.1308e-01,\n",
       "           0.0000e+00, 4.1884e-01, 0.0000e+00],\n",
       "          [0.0000e+00, 4.0171e-01, 1.8243e-01, 4.8579e-01, 4.6658e-02,\n",
       "           1.9321e-01, 1.8018e-02, 1.6586e-01],\n",
       "          [5.3419e-02, 0.0000e+00, 3.5406e-02, 0.0000e+00, 0.0000e+00,\n",
       "           3.0990e-01, 0.0000e+00, 3.6281e-01],\n",
       "          [2.6523e-01, 5.3182e-02, 5.3142e-02, 1.4562e-01, 1.3132e-02,\n",
       "           4.0218e-01, 2.0792e-01, 0.0000e+00],\n",
       "          [1.2249e-02, 0.0000e+00, 7.8594e-02, 7.2349e-02, 0.0000e+00,\n",
       "           3.8808e-02, 0.0000e+00, 1.6116e-01],\n",
       "          [0.0000e+00, 1.3675e-01, 0.0000e+00, 0.0000e+00, 1.5735e-01,\n",
       "           1.0406e-01, 1.3388e-01, 0.0000e+00]],\n",
       "\n",
       "         [[2.5242e-01, 4.1846e-01, 3.7824e-02, 0.0000e+00, 2.0024e-01,\n",
       "           0.0000e+00, 1.0493e-02, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 3.1429e-01, 0.0000e+00, 8.9888e-02,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.2922e-01, 1.6244e-01, 9.4495e-03, 6.7200e-01, 0.0000e+00,\n",
       "           9.5665e-02, 2.8730e-01, 0.0000e+00],\n",
       "          [0.0000e+00, 9.1005e-01, 3.6337e-01, 0.0000e+00, 7.9660e-02,\n",
       "           0.0000e+00, 0.0000e+00, 6.4463e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 1.7587e-01, 1.0145e+00, 8.6226e-02,\n",
       "           0.0000e+00, 0.0000e+00, 2.2060e-01],\n",
       "          [9.3059e-01, 1.3343e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 4.8916e-01, 2.0777e-01],\n",
       "          [0.0000e+00, 9.6121e-02, 6.0594e-01, 2.2441e-01, 0.0000e+00,\n",
       "           0.0000e+00, 6.9466e-01, 0.0000e+00],\n",
       "          [0.0000e+00, 1.5030e-01, 0.0000e+00, 0.0000e+00, 7.5645e-02,\n",
       "           3.1970e-02, 0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "         [[0.0000e+00, 4.3630e-01, 1.2343e-02, 0.0000e+00, 0.0000e+00,\n",
       "           7.0274e-02, 0.0000e+00, 7.6861e-02],\n",
       "          [3.6528e-01, 0.0000e+00, 3.0139e-01, 0.0000e+00, 1.6753e-01,\n",
       "           0.0000e+00, 1.8923e-01, 7.1310e-02],\n",
       "          [2.2028e-01, 3.3253e-01, 0.0000e+00, 3.9257e-01, 2.4083e-01,\n",
       "           5.7603e-02, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1116e-01, 8.1539e-04, 0.0000e+00,\n",
       "           3.5267e-01, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 3.2257e-01, 0.0000e+00, 1.7814e-01, 0.0000e+00,\n",
       "           3.4510e-01, 0.0000e+00, 7.7170e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 2.5334e-01, 0.0000e+00, 3.3459e-01,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [8.4304e-02, 3.9092e-02, 0.0000e+00, 4.2098e-01, 1.0281e-01,\n",
       "           0.0000e+00, 1.5334e-01, 7.8071e-02],\n",
       "          [3.9863e-02, 2.7263e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 2.4891e-01]],\n",
       "\n",
       "         [[1.2036e-01, 2.5020e-01, 4.2001e-01, 0.0000e+00, 4.4773e-01,\n",
       "           4.6477e-02, 0.0000e+00, 4.1128e-02],\n",
       "          [0.0000e+00, 9.7928e-02, 0.0000e+00, 9.3544e-01, 0.0000e+00,\n",
       "           5.8899e-01, 0.0000e+00, 8.7775e-02],\n",
       "          [2.7851e-01, 4.0524e-01, 4.5763e-01, 2.3800e-01, 6.5893e-01,\n",
       "           1.1773e-01, 3.2785e-01, 4.6062e-01],\n",
       "          [3.1958e-01, 3.2147e-01, 2.7071e-01, 6.2028e-02, 3.4321e-01,\n",
       "           0.0000e+00, 2.7049e-01, 2.7903e-01],\n",
       "          [8.2412e-01, 1.9482e-01, 2.0735e-01, 3.9041e-01, 3.4399e-01,\n",
       "           1.8008e-01, 2.8295e-01, 2.8061e-01],\n",
       "          [6.2172e-02, 2.0157e-01, 4.6246e-01, 6.1060e-02, 0.0000e+00,\n",
       "           8.8995e-02, 3.5040e-01, 0.0000e+00],\n",
       "          [0.0000e+00, 2.7232e-01, 4.3780e-01, 2.2767e-02, 3.1825e-01,\n",
       "           5.3867e-01, 4.1041e-02, 2.0139e-01],\n",
       "          [1.2478e-01, 7.3317e-01, 0.0000e+00, 0.0000e+00, 4.6418e-01,\n",
       "           4.5001e-01, 1.6781e-01, 9.9599e-02]],\n",
       "\n",
       "         [[0.0000e+00, 0.0000e+00, 2.9277e-01, 1.1540e-02, 0.0000e+00,\n",
       "           0.0000e+00, 1.0408e-02, 0.0000e+00],\n",
       "          [5.7279e-01, 1.3409e-01, 0.0000e+00, 0.0000e+00, 2.9924e-01,\n",
       "           2.8009e-01, 0.0000e+00, 2.3418e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1271e-01,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 7.5445e-02, 6.3609e-01, 3.6535e-01, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00]]]], device='cuda:0',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 1, 12, 12).to(device)\n",
    "model.to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"model output: {output.size()}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-dodge",
   "metadata": {},
   "source": [
    "#### Cautions\n",
    " - 모델과 인풋의 device 는 반드시 같아야 합니다.\n",
    " - 그렇지 않으면 (Runtime) Error 가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "revolutionary-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: torch.Size([1, 5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "cpu_device = torch.device('cpu')\n",
    "gpu_device = torch.device('cuda')\n",
    "\n",
    "# device is same\n",
    "dummy_input = dummy_input.to(gpu_device)\n",
    "model.to(gpu_device)\n",
    "output = model(dummy_input)  # Fine \n",
    "print(f\"model output: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacterial-event",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-52d7e79efd68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# device is different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raise Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model output: {output.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5f680354f5ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "dummy_input = dummy_input.to(cpu_device)\n",
    "model.to(gpu_device)\n",
    "\n",
    "# device is different\n",
    "# RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\n",
    "output = model(dummy_input)  # raise Error\n",
    "print(f\"model output: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-netscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "powerful-johns",
   "metadata": {},
   "source": [
    "### requires_grad()\n",
    " - autograd 가 해당 모델의 연산을 기록할지를 결정합니다\n",
    " - false 일 시, 수행하는 연산을 기록하지 않고 따라서 역전파가 되지 않아 학습에서 제외됩니다.\n",
    " - Change if autograd should record operations on parameters in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "measured-irish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param conv1.weight    required gradient? -> False\n",
      "param conv1.bias      required gradient? -> False\n",
      "param bn1.weight      required gradient? -> False\n",
      "param bn1.bias        required gradient? -> False\n",
      "param conv2.weight    required gradient? -> False\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = False\n",
    "model.requires_grad_(requires_grad=False)\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"param {param:15} required gradient? -> {weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "involved-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param conv1.weight    required gradient? -> True\n",
      "param conv1.bias      required gradient? -> True\n",
      "param bn1.weight      required gradient? -> True\n",
      "param bn1.bias        required gradient? -> True\n",
      "param conv2.weight    required gradient? -> True\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = True\n",
    "model.requires_grad_(requires_grad=True)\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"param {param:15} required gradient? -> {weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-chapter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "foreign-circus",
   "metadata": {},
   "source": [
    "### train(), eval()\n",
    " - 모델을 training(evaluation) 모드로 전환합니다.\n",
    " - training 과 evaluation 이 다르게 작용하는 모듈들(Dropout, BatchNorm) 에 영향을 줍니다.\n",
    " - 학습 단계에서는 training 모드로, 인퍼런스 단계에서는 eval 모드로 전환해주어야 합니다.\n",
    " - [아래](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L111-L118)는 BatchNorm2d 의 파이토치 구현입니다. `self.training=True` 일 경우에만, `running_mean`, `running_var` 을 tracking 합니다.\n",
    " \n",
    "```\n",
    "if self.training and self.track_running_stats:\n",
    "    # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
    "    if self.num_batches_tracked is not None:\n",
    "        self.num_batches_tracked = self.num_batches_tracked + 1\n",
    "        if self.momentum is None:  # use cumulative moving average\n",
    "            exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "        else:  # use exponential moving average\n",
    "            exponential_average_factor = self.momentum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "handed-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.bn1.training: True\n"
     ]
    }
   ],
   "source": [
    "model.train()  # set model to train mode\n",
    "print(f\"model.bn1.training: {model.bn1.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "friendly-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.bn1.training: False\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set model to eval mode\n",
    "print(f\"model.bn1.training: {model.bn1.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-transformation",
   "metadata": {},
   "source": [
    "### 파이토치 공식 문서에서 nn.Module 에 관한 더 많은 정보를 얻을 수 있습니다.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "궁금증이 생기면 공식 문서를 참고하는걸 강력 추천합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
